{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Run a prediction for a comment through a reddit hate speech model'''\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from string import punctuation\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import snowball\n",
    "import xgboost as xgb\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "stemmer = snowball.SnowballStemmer(\"english\")\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    '''Stem the tokens.'''\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    '''Tokenize & stem. Stems automatically for now.\n",
    "    Leaving \"stemmer\" out of function call, so it works with TfidfVectorizer'''\n",
    "    tokens = word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "def predict_comment(comment, classes, bst, vect):\n",
    "    '''\n",
    "    Where \"comment\" is the comment by the user, to be passed in.\n",
    "    classes =\n",
    "    '''\n",
    "    comment_tfidf = vect.transform([comment])\n",
    "    comment_xgb = xgb.DMatrix(comment_tfidf)\n",
    "    yprob = bst.predict(comment_xgb).reshape(1, 5)  # hard coding -- only one comment at a time in this case.\n",
    "    ylabel = classes[np.argmax(yprob, axis=1)]\n",
    "\n",
    "    print('The class is: {0} with probability {1}%'.format(ylabel, round(100 * np.max(yprob), 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = ['Not Hate', 'Size Hate', 'Gender Hate', 'Race Hate', 'Religion Hate']\n",
    "\n",
    "# load saved xgboost model\n",
    "bst = xgb.Booster()\n",
    "bst.load_model('../data/hatepredictor_pyladies.model')\n",
    "# load tf-idf matrix\n",
    "vect = pickle.load(open('../data/vect_pyladies.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter comment: \n",
      "The class is: Not Hate with probability 69.1%\n"
     ]
    }
   ],
   "source": [
    "# get comment from user\n",
    "comment = raw_input('Enter comment: ')\n",
    "# predict class of comment\n",
    "predict_comment(comment, classes, bst, vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HateModel = gensim.models.Word2Vec.load('../data/w2vHateModel.model')\n",
    "NotHateModel = gensim.models.Word2Vec.load('../Data/w2vNotHateModel.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_most_sim(words, HateModel, NotHateModel):\n",
    "    '''words is a list of words'''\n",
    "    for word in words:\n",
    "        nothatelist = NotHateModel.most_similar(word)\n",
    "        hatelist = HateModel.most_similar(word)\n",
    "        print ' '\n",
    "        print \"Not Hate           word='{0}'               Hate\".format(word)\n",
    "        print \"---------------------------------------------------\"\n",
    "        for nh, h in zip(nothatelist, hatelist):\n",
    "            print \"{0}\\t\\t{1}\".format(nh, h)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = ['fat', 'dress', 'pretty', 'suck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Not Hate           word='dress'               Hate\n",
      "---------------------------------------------------\n",
      "(u'steak', 0.5374936461448669)\t\t(u'soldier', 0.6863648891448975)\n",
      "(u'wear', 0.5334213376045227)\t\t(u'retard', 0.664968729019165)\n",
      "(u'shop', 0.4940866231918335)\t\t(u'gut', 0.654162585735321)\n",
      "(u'taste', 0.4924619197845459)\t\t(u'bowling', 0.651995062828064)\n",
      "(u'calculus', 0.49234992265701294)\t\t(u'linebacker', 0.6515874862670898)\n",
      "(u'coffee', 0.489260196685791)\t\t(u'slut', 0.6515154838562012)\n",
      "(u'makeup', 0.47544825077056885)\t\t(u'toad', 0.6447991728782654)\n",
      "(u'balloon', 0.47303318977355957)\t\t(u'fold', 0.6412398815155029)\n",
      "(u'referral', 0.4679303467273712)\t\t(u'rock', 0.6399352550506592)\n",
      "(u'counselor', 0.46694833040237427)\t\t(u'bloody', 0.6364386081695557)\n",
      " \n",
      "Not Hate           word='fat'               Hate\n",
      "---------------------------------------------------\n",
      "(u'overweight', 0.6250482797622681)\t\t(u'skinny', 0.6153160333633423)\n",
      "(u'healthy', 0.6066110730171204)\t\t(u'thin', 0.6055712699890137)\n",
      "(u'obese', 0.6034889221191406)\t\t(u'lazy', 0.5896681547164917)\n",
      "(u'thin', 0.6032106876373291)\t\t(u'shaming', 0.5388163328170776)\n",
      "(u'skinny', 0.572454571723938)\t\t(u'ugly', 0.5361239910125732)\n",
      "(u'normal', 0.5489888191223145)\t\t(u'grandmas', 0.5138142704963684)\n",
      "(u'diet', 0.5401106476783752)\t\t(u'obese', 0.5042471885681152)\n",
      "(u'disordered', 0.5233265161514282)\t\t(u'disabled', 0.49886980652809143)\n",
      "(u'shaming', 0.5193393230438232)\t\t(u'delusional', 0.4923228621482849)\n",
      "(u'unhealthy', 0.5189762711524963)\t\t(u'bashing', 0.4703002870082855)\n",
      " \n",
      "Not Hate           word='pretty'               Hate\n",
      "---------------------------------------------------\n",
      "(u'fairly', 0.632055401802063)\t\t(u'too', 0.69933021068573)\n",
      "(u'very', 0.5489404797554016)\t\t(u'very', 0.6474765539169312)\n",
      "(u'too', 0.5270552635192871)\t\t(u'so', 0.5795998573303223)\n",
      "(u'super', 0.5148956179618835)\t\t(u'super', 0.5563167333602905)\n",
      "(u'incredibly', 0.4881615936756134)\t\t(u'poly', 0.5511360168457031)\n",
      "(u'extremely', 0.48637035489082336)\t\t(u'fairly', 0.5374546051025391)\n",
      "(u'becuase', 0.46936485171318054)\t\t(u'supper', 0.5344796180725098)\n",
      "(u'huck', 0.45581257343292236)\t\t(u'tho', 0.5191142559051514)\n",
      "(u'sooooo', 0.45320507884025574)\t\t(u'tbh', 0.501793622970581)\n",
      "(u'abundantly', 0.45048773288726807)\t\t(u'quite', 0.500005841255188)\n",
      " \n",
      "Not Hate           word='suck'               Hate\n",
      "---------------------------------------------------\n",
      "(u'knock', 0.6139926910400391)\t\t(u'cook', 0.7131173610687256)\n",
      "(u'chalk', 0.5893340706825256)\t\t(u'lay', 0.698693037033081)\n",
      "(u'sew', 0.5883692502975464)\t\t(u'stink', 0.6886621713638306)\n",
      "(u'dig', 0.5780432820320129)\t\t(u'appointments', 0.6854016780853271)\n",
      "(u'straighten', 0.5778012275695801)\t\t(u'skip', 0.6832385063171387)\n",
      "(u'shake', 0.5703983306884766)\t\t(u'struggled', 0.6769531965255737)\n",
      "(u'screw', 0.5522549748420715)\t\t(u'waddle', 0.6762340664863586)\n",
      "(u'flick', 0.5519547462463379)\t\t(u'wipe', 0.668322741985321)\n",
      "(u'shrug', 0.5465000867843628)\t\t(u'inject', 0.6681497693061829)\n",
      "(u'blow', 0.5399208068847656)\t\t(u'bust', 0.6681323051452637)\n"
     ]
    }
   ],
   "source": [
    "print_most_sim(words, HateModel, NotHateModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
